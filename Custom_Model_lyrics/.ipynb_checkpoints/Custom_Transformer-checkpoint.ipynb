{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5344ee5b34c74eff9797d4f509744053",
      "723ae4c9989d4676a91b022fe23d12f0",
      "30622971c7814fdb899660f789004623",
      "d55db9e9f41246e88d7dbb5390e7eb91",
      "6efa1afa63f74dec8694ac4dba869db2",
      "61e77f0d3d1043feb118d2eee30b0ef8",
      "725b98b1edce43689ec7f704a262e6e4",
      "485cb13f64484c7391f7a6d16d33ad23",
      "fa73585ff1564ca0a28453573cacb69f",
      "41d29751de0e4c3587c6e98c43331fdf",
      "849eaa0f87a54f96ae9e12fcb9f23a2b",
      "b12f8f2c9eb44899b9598b20b9307356",
      "9e773f7e81674cebb65a13332f3bff7d",
      "45dea46d0b3648739dcca3cde19655d3",
      "e0bf6a12069748ce9b72269911f87e35",
      "afc97ed693cd4bcbbdc3c5027eb69b77",
      "7d651f1667f74a38a35c662530ee2161",
      "8630d427eae24c11987029fe54e72092",
      "a16847e8c842492e9bd1825b1ac5284c",
      "46b4f6c3796e4b8aab4d62d4fbad4b27",
      "9c34a3b4b0fc48a0a959b08aa5479ff7",
      "75e9e4e77e844f0eb5f535402f1d8544",
      "b19888718df942a8b98595c6691d514c",
      "12b78775f3ab49679d6db4bfd4188285",
      "d9fa2a28a41b44119cb04425bc0626fc",
      "f1793cabd34242b984edba931196576a",
      "fc8543f3def74738aeaf25ab4702dae0",
      "0df43236fabd449385bcc5799a3e2ac1",
      "dad279ee8658496fa72e796f7bca2121",
      "52d1d6488d9a4b55ab67ea4589bd0eb6",
      "437e01cdc35f404bbc676125b49b89ec",
      "b19f846bb67343e9a2dbe3fff6518ca5",
      "5deea358c1614eb98e00e0e26b9ef6c2",
      "56872d0a4096420ba74297ce5b8fdcda",
      "55ed1b1e90dc416f95cc1e6e539145b6",
      "4dcf5eb6dacd473b8d8c755c5a1c3fcc",
      "6e2cba35d8194bb0ae95ad52916b925a",
      "1a5b451e03494cad94dbe96c894f2524",
      "b9336d3f18b14eb1ae03ce94e8784212",
      "5d4e4c4f64084715990653c66feaf6d5",
      "5e37c3372fa24c00afa959f9cc0ef8f7",
      "add4eae012454a968aa0bf5e8c00d099",
      "ac0165fb453147adbb897ca9b620e101",
      "b7d10011a9ec4cc2ab67e44778b32cd8"
     ]
    },
    "id": "w4Tntx9-xajc",
    "outputId": "163a2f51-4850-40f5-aec6-4f3e3b04e416"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Attempting to load dataset: bookcorpus\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5344ee5b34c74eff9797d4f509744053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12f8f2c9eb44899b9598b20b9307356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bookcorpus.py:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19888718df942a8b98595c6691d514c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56872d0a4096420ba74297ce5b8fdcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/74004228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing up to 500000 samples from dataset...\n",
      "Using 'text' as the text field\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:   1%|          | 571703/74004228 [00:11<23:56, 51123.98it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500000 texts\n",
      "Successfully loaded 500000 samples from bookcorpus\n",
      "Initializing tokenizer...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 500000/500000 [00:02<00:00, 194326.85it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 500000/500000 [01:12<00:00, 6870.50it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size: 8\n",
      "Training samples: 450000, Validation samples: 50000\n",
      "Model size: 3.43M parameters\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 56250/56250 [35:54<00:00, 26.10it/s]\n",
      "Validation: 100%|██████████| 6250/6250 [01:26<00:00, 72.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 3.8106, Val Loss: 1.3019, LR: 0.000100\n",
      "Saved best model with validation loss: 1.3019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 56250/56250 [36:06<00:00, 25.97it/s]\n",
      "Validation: 100%|██████████| 6250/6250 [01:26<00:00, 72.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 0.8826, Val Loss: 0.4150, LR: 0.000100\n",
      "Saved best model with validation loss: 0.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 56250/56250 [36:04<00:00, 25.99it/s]\n",
      "Validation: 100%|██████████| 6250/6250 [01:27<00:00, 71.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 0.5198, Val Loss: 0.2891, LR: 0.000100\n",
      "Saved best model with validation loss: 0.2891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'EnhancedTokenizer.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7576871aff80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-7576871aff80>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     torch.save({\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'EnhancedTokenizer.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Enhanced Tokenizer with Markov Chain for Better Word Prediction\n",
    "class EnhancedTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.markov_chains = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def fit(self, texts):\n",
    "        words = []\n",
    "        for text in tqdm(texts, desc=\"Building vocabulary\"):\n",
    "            words.extend(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        common_words = ['<pad>', '<unk>'] + [word for word, _ in word_counts.most_common(self.vocab_size - 2)]\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(common_words)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "        # Build Markov chain for transition probabilities\n",
    "        for text in texts:\n",
    "            word_list = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            for i in range(len(word_list) - 1):\n",
    "                self.markov_chains[word_list[i]][word_list[i + 1]] += 1\n",
    "\n",
    "        # Normalize transition probabilities\n",
    "        for word in self.markov_chains:\n",
    "            total = sum(self.markov_chains[word].values())\n",
    "            if total > 0:\n",
    "                for next_word in self.markov_chains[word]:\n",
    "                    self.markov_chains[word][next_word] /= total\n",
    "\n",
    "    def encode(self, text, max_length=512):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        # Avoid unknown tokens by filtering out words not in vocab\n",
    "        ids = [self.word_to_idx.get(word, self.pad_token_id) for word in words if word in self.word_to_idx]\n",
    "        if not ids:\n",
    "            ids = [self.pad_token_id]\n",
    "\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_length]\n",
    "\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = [self.idx_to_word.get(id.item(), '') for id in ids if id != self.pad_token_id and id in self.idx_to_word]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def suggest_next_word(self, current_word):\n",
    "        if current_word in self.markov_chains and self.markov_chains[current_word]:\n",
    "            next_words = list(self.markov_chains[current_word].keys())\n",
    "            probs = list(self.markov_chains[current_word].values())\n",
    "            return random.choices(next_words, weights=probs, k=1)[0]\n",
    "        return random.choice(list(self.word_to_idx.keys())[2:])  # Exclude pad and unk\n",
    "\n",
    "# Transformer Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        context = context.reshape(batch_size, seq_len, d_model)\n",
    "        return self.proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Main Model\n",
    "class LyricsTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=4, d_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 512, d_model))\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dataset\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, texts, input_ids, target_ids):\n",
    "        self.texts = texts\n",
    "        self.input_ids = input_ids\n",
    "        self.target_ids = target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'target_ids': self.target_ids[idx]\n",
    "        }\n",
    "\n",
    "# Training function with gradient accumulation\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=3, gradient_accumulation_steps=4):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98))\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                target_ids = batch['target_ids'].to(device)\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_lyrics_model.pth')\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Generate text with repetition control and Markov guidance\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, max_length=512).unsqueeze(0).to(device)\n",
    "    generated = []\n",
    "    seen_words = set()\n",
    "    max_seq_length = 512\n",
    "    min_tokens = 10  # Ensure at least 10 tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if input_ids.size(1) > max_seq_length:\n",
    "                input_ids = input_ids[:, -max_seq_length:]\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).to(device)\n",
    "\n",
    "            if len(generated) >= min_tokens and next_token.item() == tokenizer.pad_token_id:\n",
    "                break\n",
    "\n",
    "            word = tokenizer.idx_to_word.get(next_token.item(), '<unk>')\n",
    "            if word not in seen_words or len(generated) < min_tokens:  # Allow repeats only if below min_tokens\n",
    "                generated.append(next_token.item())\n",
    "                seen_words.add(word)\n",
    "                next_word_suggestion = tokenizer.suggest_next_word(word)\n",
    "                if next_word_suggestion and next_word_suggestion not in seen_words:\n",
    "                    next_token = torch.tensor([tokenizer.word_to_idx.get(next_word_suggestion, tokenizer.pad_token_id)]).to(device)\n",
    "            else:\n",
    "                # Try to find an unseen word using Markov suggestion\n",
    "                next_word = tokenizer.suggest_next_word(tokenizer.idx_to_word.get(generated[-1], ''))\n",
    "                while next_word in seen_words and len(seen_words) < len(tokenizer.word_to_idx) - 2:\n",
    "                    next_word = tokenizer.suggest_next_word(next_word)\n",
    "                next_token = torch.tensor([tokenizer.word_to_idx.get(next_word, tokenizer.pad_token_id)]).to(device)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    result = prompt + ' ' + tokenizer.decode(torch.tensor(generated))\n",
    "    return result\n",
    "\n",
    "# Data processing\n",
    "def process_text_dataset(dataset, max_samples=500000):\n",
    "    print(f\"Processing up to {max_samples} samples from dataset...\")\n",
    "    texts = []\n",
    "    counter = 0\n",
    "\n",
    "    if hasattr(dataset, 'keys'):\n",
    "        splits = list(dataset.keys())\n",
    "    else:\n",
    "        splits = ['dataset']\n",
    "        dataset = {'dataset': dataset}\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = dataset[split]\n",
    "        if len(split_data) > 0:\n",
    "            sample = split_data[0]\n",
    "            text_fields = [key for key, value in sample.items() if isinstance(value, str) and len(value.split()) > 5]\n",
    "            text_field = next((field for field in ['text', 'content', 'lyrics', 'sentence', 'article'] if field in text_fields), text_fields[0] if text_fields else None)\n",
    "\n",
    "            if text_field:\n",
    "                print(f\"Using '{text_field}' as the text field\")\n",
    "                for item in tqdm(split_data, desc=f\"Processing {split}\"):\n",
    "                    if text_field in item and item[text_field] and isinstance(item[text_field], str) and len(item[text_field].strip().split()) > 5:\n",
    "                        texts.append(item[text_field])\n",
    "                        counter += 1\n",
    "                        if counter >= max_samples:\n",
    "                            break\n",
    "            else:\n",
    "                print(f\"Could not identify a text field in the dataset. Sample keys: {list(sample.keys())}\")\n",
    "\n",
    "        if counter >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(f\"Processed {len(texts)} texts\")\n",
    "    return texts\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device.type == 'cpu':\n",
    "        print(\"Warning: Running on CPU. Training will be slow.\")\n",
    "\n",
    "    max_samples = 500000\n",
    "    dataset_options = [\n",
    "        \"bookcorpus\",\n",
    "        \"wikitext\",\n",
    "        \"Abirate/english_quotes\",\n",
    "        \"imdb\",\n",
    "        \"ag_news\",\n",
    "        \"glue/sst2\"\n",
    "    ]\n",
    "\n",
    "    texts = []\n",
    "    for dataset_name in dataset_options:\n",
    "        try:\n",
    "            print(f\"Attempting to load dataset: {dataset_name}\")\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "            texts = process_text_dataset(dataset, max_samples=max_samples)\n",
    "            if texts:\n",
    "                print(f\"Successfully loaded {len(texts)} samples from {dataset_name}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not texts:\n",
    "        print(\"All dataset attempts failed. Using wikitext-103-v1 as a fallback.\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
    "            texts = [item[\"text\"] for item in dataset if item[\"text\"] and len(item[\"text\"].strip()) > 50]\n",
    "            if len(texts) > max_samples:\n",
    "                texts = texts[:max_samples]\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback failed: {e}\")\n",
    "            return\n",
    "\n",
    "    print(\"Initializing tokenizer...\")\n",
    "    tokenizer = EnhancedTokenizer(vocab_size=10000)\n",
    "    tokenizer.fit(texts)\n",
    "\n",
    "    print(\"Preparing datasets...\")\n",
    "    max_length = 512\n",
    "    all_input_ids = []\n",
    "    all_target_ids = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Encoding texts\"):\n",
    "        input_id = tokenizer.encode(text, max_length)\n",
    "        target_id = input_id.clone()\n",
    "        target_id[:-1] = input_id[1:]\n",
    "        target_id[-1] = input_id[0]\n",
    "        all_input_ids.append(input_id)\n",
    "        all_target_ids.append(target_id)\n",
    "\n",
    "    indices = list(range(len(texts)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_dataset = LyricsDataset(\n",
    "        [texts[i] for i in train_indices],\n",
    "        [all_input_ids[i] for i in train_indices],\n",
    "        [all_target_ids[i] for i in train_indices]\n",
    "    )\n",
    "    val_dataset = LyricsDataset(\n",
    "        [texts[i] for i in val_indices],\n",
    "        [all_input_ids[i] for i in val_indices],\n",
    "        [all_target_ids[i] for i in val_indices]\n",
    "    )\n",
    "\n",
    "    batch_size = 16 if device.type == 'cuda' else 4\n",
    "    if len(train_dataset) > 100000:\n",
    "        batch_size = min(batch_size, 8)\n",
    "    num_workers = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    model = LyricsTransformer(\n",
    "        vocab_size=10000,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        d_ff=512\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        num_epochs=3,\n",
    "        gradient_accumulation_steps=4\n",
    "    )\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'tokenizer': tokenizer\n",
    "    }, 'final_lyrics_model.pth')\n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "    print(\"\\nGenerating sample text...\")\n",
    "    prompt = \"Write a song about love and romance of my life:\"\n",
    "    generated_text = generate_text(model, tokenizer, prompt, device=device)\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660,
     "referenced_widgets": [
      "0e308b60314843de982bba60c1b3cca0",
      "e82acd3405ed47dcacd0d1583963b909",
      "bfaa2aba830a422c96d7e3a2ce4acae8",
      "eca2598e806e41fdab2539ed72f80e58",
      "2c195c023dcd4ab8987fb3eca3f7c592",
      "23548116836246eba5a817378f3222dd",
      "5e8624e9541243efa7408a25d8c436e2",
      "6ba3a54733d54151b42040232ec3dd15",
      "347b4a53d2e64b1296bb44a34d26a73d",
      "77215c54c0d54345b2113f1bf1e135fb",
      "9fa28c2266d749728e4339c41fcd9f19",
      "94323561a2bc49cb900a7cee01ec492c",
      "8dd70da844404e0093d4fb1c8d8f8a20",
      "b6b6d325f16343f5afac5b583debbbdb",
      "a173346807904a738070e9c30e5fd51e",
      "e685a24dbcfa444d8cbb6ed1abe3b327",
      "ba3ff506573d4e37bb7731e5c456cbaf",
      "2676d305c128479589a93565c779588e",
      "ce69b49e00d143f6b81d7f2d06fec378",
      "d4daefb22efc47b3bdb607512147cd81",
      "130eed6df78f4e568df2d1ed8cb8e6d8",
      "2aeb0aa31a2b46a68db2987f84eefb34",
      "fcf263ee4e504219b0036a8d04c78fc8",
      "717ffc6a2b314b6f9c656736b9de44dd",
      "1ca6044323fc4dfa81d6356f3b22a255",
      "8be3ee4139d04a4198e4e964d6755ecd",
      "62f190ffb66247fda59f0b633beae9b3",
      "68de9129d69b467c954bac2fe9e584b5",
      "6a0508863cf346b2b44aa2aa6e323d10",
      "fcb0ba80691843798b5e371de5c4fa49",
      "a4b779b171024e7a916f92099af5966b",
      "eb99894a782d418fac67e23da1945818",
      "7833d081956147e3a16e7cdebf6e67f7",
      "f43082c8ea3e4642bc7b22b4a3a0f93f",
      "9ab1e9a090df443486259923b2d88d7f",
      "7d4b3dc79e114dba975f8999abe309d5",
      "1cf58a726b384f7b8a9e792ed7ee5ec2",
      "5e7d29575e14434ead8ed64d09338b12",
      "0bb3d91de90449bba05a6f4c75dc2d43",
      "ee4b27941b994d8a82bedcdc034473c7",
      "459251b662154788a900005160f1f064",
      "04e3389a7e054d8ca8aaf0f583e44668",
      "e6dbc347f1aa460c91c50e86e229bf2c",
      "b7fdaf8f0b58451a8f20022393c5252e",
      "847c9ad4378148fdbf782b6c9d2441d8",
      "1daedf6b7e6040bda318fb900f00067a",
      "79a6468e230f41409bbce8a623959b3c",
      "cdad806fa560495bbc4edea1de382c97",
      "cc151ab7409b469cade2848294e38a3f",
      "ffe69b4f25094eb59506ee16a4d2eceb",
      "2834b66bcbb8457389b94c47c475e233",
      "2077a92ff30049ffa380c44846b1a4df",
      "679ce78a56174bb9be74b2117b2842ed",
      "289431d93ad44fcc8b8d6e30f3889beb",
      "9743ce89a11b4d249bbcac13f185ae40",
      "61a0c104eba64f3ab5dd2b69c6300e54",
      "ff85e6a4c1784d558f370f9b46baefc9",
      "9890948b18604dad86887240c9859d31",
      "af25bf85691240d5a4be7cb0a227aaa9",
      "794f64e69e8e49a2a493048cc290b538",
      "448449b59aac4b2e97e1e88361f93a73",
      "88824f06498a4770a205a13a92711f61",
      "b703dc00e0b1485f81e1602899347557",
      "81fcb45a1c74434da2261abd71a5302a",
      "7350f829600646ec9a0b6993de2e4acd",
      "75d0b30bd69c4f94a5d34f660f2cde04",
      "a65e8b147e7d4c5b89832545dbec51c5",
      "b76b665fd16a4a4fb0213c41abbdc424",
      "3be60cad87cd45e39eee6d64416c8e12",
      "563cc5da780b4b3f85ed35e3940bb775",
      "6440d52bdb834f75940ce372c56f49ad",
      "c168c3c6550845f18a60b55506112f2a",
      "eb350264d7df44508d909ed59342c38a",
      "c1c5deefd3ca45f0a4647bbad1e22bb1",
      "012d4d9e6dd7490cbda4184eb0f48d90",
      "145c7c06f3b7428b8b5a42c7c247bbd3",
      "26fa79b8e222484ba6915366b3059672"
     ]
    },
    "id": "ge6HHb1txni8",
    "outputId": "153b286f-50bd-4e55-cb3b-e2e595858015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Attempting to load dataset: rotten_tomatoes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95959a4b43749c2bd9085a545c605cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kiki\\.cache\\huggingface\\hub\\datasets--rotten_tomatoes. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3530e4d635884261bcd35fa0ae2b09f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e50f664b393494290b38d1d58da377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41346c4f6a9a47a5ab4571e3bc9cc39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9c3b64088a48db87fcfc921a4c0624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1c6156f3f140aaaae373561d4805ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7a66d987f1447ea263f3c58f4a50ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|███████████████████████████████████████████████████████| 8530/8530 [00:00<00:00, 10972.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 8388 samples from rotten_tomatoes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████████████████████████████████████████████████| 8388/8388 [00:00<00:00, 45570.36it/s]\n",
      "Building Markov chain: 100%|█████████████████████████████████████████████████████| 8388/8388 [00:02<00:00, 3061.09it/s]\n",
      "Encoding texts: 100%|████████████████████████████████████████████████████████████| 8388/8388 [00:02<00:00, 3284.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size: 4\n",
      "Training samples: 7549, Validation samples: 839\n",
      "Model size: 0.13M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████| 1888/1888 [00:58<00:00, 32.45it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 210/210 [00:02<00:00, 90.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.3556, Val Loss: 0.1595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████| 1888/1888 [00:55<00:00, 34.22it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 210/210 [00:02<00:00, 93.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 0.1532, Val Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████| 1888/1888 [00:54<00:00, 34.35it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 210/210 [00:02<00:00, 99.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 0.1399, Val Loss: 0.1338\n",
      "Training completed and model saved!\n",
      "\n",
      "Generating sample text...\n",
      "Write a song about love and romance of my life: always rise shadow <unk> <unk> pain hold smile <unk> come leave never touch fire sky voice light <unk> time night sun soul dreams joy smile joy free <unk> song run go leave mind joy sing always sing hands rhyme life song night time fly hold break sing sing shine never never eyes come rhythm fall <unk> rhythm day hold heart road break night forever memory star smile darling together always light moon rise sky hands together apart dreams fall hold <unk> baby fire smile way cry hands <unk> dark road hands back eyes <unk> heart fall fall know <unk> break\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "# Common words for lyrics (curated to reduce <unk> and improve lyrical quality)\n",
    "COMMON_LYRICS_WORDS = [\n",
    "    '<pad>', '<unk>', 'love', 'heart', 'dream', 'night', 'day', 'life', 'time', 'way',\n",
    "    'feel', 'know', 'go', 'see', 'world', 'soul', 'sky', 'star', 'moon', 'sun',\n",
    "    'baby', 'darling', 'forever', 'always', 'never', 'together', 'apart', 'home', 'road', 'fire',\n",
    "    'dance', 'sing', 'song', 'melody', 'rhyme', 'beat', 'rhythm', 'free', 'run', 'fly',\n",
    "    'hold', 'touch', 'kiss', 'smile', 'cry', 'tears', 'pain', 'joy', 'hope', 'fear',\n",
    "    'light', 'dark', 'shadow', 'shine', 'burn', 'break', 'fall', 'rise', 'stay', 'leave',\n",
    "    'come', 'gone', 'back', 'memory', 'dreams', 'eyes', 'hands', 'voice', 'mind', 'spirit'\n",
    "]\n",
    "\n",
    "# Custom Tokenizer with Lyrics-Specific Vocabulary\n",
    "class LyricsTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(COMMON_LYRICS_WORDS)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        words = []\n",
    "        for text in tqdm(texts, desc=\"Building vocabulary\"):\n",
    "            words.extend(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        additional_words = [word for word, _ in word_counts.most_common(self.vocab_size - len(COMMON_LYRICS_WORDS))]\n",
    "        for word in additional_words:\n",
    "            if word not in self.word_to_idx and len(self.word_to_idx) < self.vocab_size:\n",
    "                self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = word\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text, max_length=256):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        ids = [self.word_to_idx.get(word, self.unk_token_id) for word in words]\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_length]\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = [self.idx_to_word.get(id.item(), '<unk>') for id in ids if id != self.pad_token_id]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Markov Chain for Coherence\n",
    "class MarkovChain:\n",
    "    def __init__(self, order=2):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(list)\n",
    "\n",
    "    def fit(self, texts, tokenizer):\n",
    "        for text in tqdm(texts, desc=\"Building Markov chain\"):\n",
    "            ids = tokenizer.encode(text).tolist()\n",
    "            for i in range(len(ids) - self.order):\n",
    "                state = tuple(ids[i:i + self.order])\n",
    "                next_token = ids[i + self.order]\n",
    "                self.transitions[state].append(next_token)\n",
    "\n",
    "    def get_next_token(self, state, vocab_size):\n",
    "        state = tuple(state[-self.order:])\n",
    "        if state in self.transitions and self.transitions[state]:\n",
    "            return np.random.choice(self.transitions[state])\n",
    "        return np.random.randint(2, vocab_size)  # Avoid <pad> and <unk>\n",
    "\n",
    "# Transformer Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        return self.proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Simplified Transformer Model\n",
    "class LyricsTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 256, d_model))\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dataset\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, texts, input_ids, target_ids):\n",
    "        self.texts = texts\n",
    "        self.input_ids = input_ids\n",
    "        self.target_ids = target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'target_ids': self.target_ids[idx]\n",
    "        }\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, device, markov_chain, tokenizer, num_epochs=3, gradient_accumulation_steps=4):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                target_ids = batch['target_ids'].to(device)\n",
    "                outputs = model(input_ids)\n",
    "                val_loss += criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1)).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_lyrics_model.pth')\n",
    "\n",
    "# Hybrid Generation Function\n",
    "def generate_text(model, tokenizer, markov_chain, prompt, max_length=100, temperature=0.8, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, max_length=256).unsqueeze(0).to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    max_seq_length = 256\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(generated) > max_seq_length:\n",
    "                input_ids = torch.tensor(generated[-max_seq_length:]).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                input_ids = torch.tensor(generated).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[0, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Use Markov chain to guide sampling\n",
    "            markov_suggestion = markov_chain.get_next_token(generated, tokenizer.vocab_size)\n",
    "            markov_boost = torch.zeros_like(probs)\n",
    "            markov_boost[markov_suggestion] += 0.3  # Boost probability of Markov suggestion\n",
    "            probs = F.softmax(probs + markov_boost, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            if next_token == tokenizer.pad_token_id:\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "\n",
    "    return prompt + ' ' + tokenizer.decode(torch.tensor(generated[len(tokenizer.encode(prompt)):]))\n",
    "\n",
    "# Data Processing\n",
    "def process_text_dataset(dataset, max_samples=100000):\n",
    "    texts = []\n",
    "    counter = 0\n",
    "    splits = dataset.keys() if hasattr(dataset, 'keys') else ['dataset']\n",
    "    dataset = dataset if hasattr(dataset, 'keys') else {'dataset': dataset}\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = dataset[split]\n",
    "        sample = split_data[0]\n",
    "        text_fields = [key for key, value in sample.items() if isinstance(value, str) and len(value.split()) > 5]\n",
    "        text_field = next((field for field in ['text', 'content', 'lyrics', 'sentence', 'article'] if field in text_fields), text_fields[0] if text_fields else None)\n",
    "\n",
    "        if text_field:\n",
    "            for item in tqdm(split_data, desc=f\"Processing {split}\"):\n",
    "                if text_field in item and item[text_field] and isinstance(item[text_field], str) and len(item[text_field].strip().split()) >= 5:\n",
    "                    texts.append(item[text_field])\n",
    "                    counter += 1\n",
    "                    if counter >= max_samples:\n",
    "                        break\n",
    "        if counter >= max_samples:\n",
    "            break\n",
    "\n",
    "    return texts\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    max_samples = 100000\n",
    "    dataset_options = [\"rotten_tomatoes\", \"bookcorpus\", \"wikitext\", \"Abirate/english_quotes\", \"imdb\", \"ag_news\", \"glue/sst2\"]\n",
    "\n",
    "    texts = []\n",
    "    for dataset_name in dataset_options:\n",
    "        try:\n",
    "            print(f\"Attempting to load dataset: {dataset_name}\")\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "            texts = process_text_dataset(dataset, max_samples=max_samples)\n",
    "            if texts:\n",
    "                print(f\"Successfully loaded {len(texts)} samples from {dataset_name}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not texts:\n",
    "        print(\"All dataset attempts failed.\")\n",
    "        return\n",
    "\n",
    "    tokenizer = LyricsTokenizer(vocab_size=10000)\n",
    "    tokenizer.fit(texts)\n",
    "\n",
    "    markov_chain = MarkovChain(order=2)\n",
    "    markov_chain.fit(texts, tokenizer)\n",
    "\n",
    "    max_length = 256\n",
    "    all_input_ids = []\n",
    "    all_target_ids = []\n",
    "    for text in tqdm(texts, desc=\"Encoding texts\"):\n",
    "        input_id = tokenizer.encode(text, max_length)\n",
    "        target_id = input_id.clone()\n",
    "        target_id[:-1] = input_id[1:]\n",
    "        target_id[-1] = input_id[0]\n",
    "        all_input_ids.append(input_id)\n",
    "        all_target_ids.append(target_id)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(list(range(len(texts))), test_size=0.1, random_state=42)\n",
    "    train_dataset = LyricsDataset(\n",
    "        [texts[i] for i in train_indices],\n",
    "        [all_input_ids[i] for i in train_indices],\n",
    "        [all_target_ids[i] for i in train_indices]\n",
    "    )\n",
    "    val_dataset = LyricsDataset(\n",
    "        [texts[i] for i in val_indices],\n",
    "        [all_input_ids[i] for i in val_indices],\n",
    "        [all_target_ids[i] for i in val_indices]\n",
    "    )\n",
    "\n",
    "    batch_size = 8 if device.type == 'cuda' else 4\n",
    "    num_workers = 2 if device.type == 'cuda' else 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    model = LyricsTransformer(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        d_ff=256\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        markov_chain,\n",
    "        tokenizer,\n",
    "        num_epochs=3,\n",
    "        gradient_accumulation_steps=4\n",
    "    )\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'tokenizer': tokenizer,\n",
    "        'markov_chain': markov_chain\n",
    "    }, 'final_lyrics_model.pth')\n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "    print(\"\\nGenerating sample text...\")\n",
    "    prompt = \"Write a song about love and romance of my life:\"\n",
    "    generated_text = generate_text(model, tokenizer, markov_chain, prompt, device=device)\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9NUNbI_xpR-",
    "outputId": "68c02a8c-0b65-4743-dff1-6e1a1e888373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a theme for your song (love, nature, friendship, hope, freedom): love\n",
      "\n",
      "Loading custom lyrics generator model...\n",
      "\n",
      "Successfully loaded checkpoint\n",
      "Loaded tokenizer from checkpoint\n",
      "Loaded MarkovChain from checkpoint\n",
      "Loaded model weights from 'model_state_dict' key\n",
      "Model loaded successfully from /content/final_lyrics_model.pth\n",
      "\n",
      "Generating Lyrics... Please wait.\n",
      "\n",
      "Using device: cuda\n",
      "Generated Lyrics:\n",
      "\n",
      "Verse 1:\n",
      "<unk> joy tears life song leave melody hope\n",
      "light eyes baby cry pain gone voice dance\n",
      "together come sing sun sun sing kiss touch\n",
      "break touch rhythm come time go life darling\n",
      "\n",
      "Chorus:\n",
      "leave home voice dream forever see kiss home\n",
      "life apart leave dreams joy rise fly come\n",
      "sing song dream rhythm apart love break dreams\n",
      "know heart hope eyes feel world apart <unk>\n",
      "\n",
      "Speech generated and saved as 'lyrics_audio.mp3'\n",
      "\n",
      "Enter path to melody file (default: generated_melody.wav): /content/generated_melody.wav\n",
      "\n",
      "Combining lyrics with melody...\n",
      "\n",
      "Final combined song saved as 'love_song.mp3'\n",
      "\n",
      "Song generation complete! Your song is saved as 'love_song.mp3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the themes and the prompt for each theme\n",
    "themes = {\n",
    "    \"love\": \"Write a beautiful song about love and relationships.\",\n",
    "    \"nature\": \"Write a song about the beauty of nature and the environment.\",\n",
    "    \"friendship\": \"Write a song about the power and importance of friendship.\",\n",
    "    \"hope\": \"Write a song about hope and positivity for the future.\",\n",
    "    \"freedom\": \"Write a song about freedom and independence.\"\n",
    "}\n",
    "\n",
    "# Lyrics-specific vocabulary\n",
    "COMMON_LYRICS_WORDS = [\n",
    "    '<pad>', '<unk>', 'love', 'heart', 'dream', 'night', 'day', 'life', 'time', 'way',\n",
    "    'feel', 'know', 'go', 'see', 'world', 'soul', 'sky', 'star', 'moon', 'sun',\n",
    "    'baby', 'darling', 'forever', 'always', 'never', 'together', 'apart', 'home', 'road', 'fire',\n",
    "    'dance', 'sing', 'song', 'melody', 'rhyme', 'beat', 'rhythm', 'free', 'run', 'fly',\n",
    "    'hold', 'touch', 'kiss', 'smile', 'cry', 'tears', 'pain', 'joy', 'hope', 'fear',\n",
    "    'light', 'dark', 'shadow', 'shine', 'burn', 'break', 'fall', 'rise', 'stay', 'leave',\n",
    "    'come', 'gone', 'back', 'memory', 'dreams', 'eyes', 'hands', 'voice', 'mind', 'spirit'\n",
    "]\n",
    "\n",
    "# Custom Tokenizer (matching the training code)\n",
    "class LyricsTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(COMMON_LYRICS_WORDS)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        words = []\n",
    "        for text in tqdm(texts, desc=\"Building vocabulary\"):\n",
    "            words.extend(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        additional_words = [word for word, _ in word_counts.most_common(self.vocab_size - len(COMMON_LYRICS_WORDS))]\n",
    "        for word in additional_words:\n",
    "            if word not in self.word_to_idx and len(self.word_to_idx) < self.vocab_size:\n",
    "                self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = word\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text, max_length=256):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        ids = [self.word_to_idx.get(word, self.unk_token_id) for word in words]\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_length]\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = [self.idx_to_word.get(id.item(), '<unk>') for id in ids if id != self.pad_token_id]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Markov Chain (matching the training code)\n",
    "class MarkovChain:\n",
    "    def __init__(self, order=2):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(list)\n",
    "\n",
    "    def fit(self, texts, tokenizer):\n",
    "        for text in tqdm(texts, desc=\"Building Markov chain\"):\n",
    "            ids = tokenizer.encode(text).tolist()\n",
    "            for i in range(len(ids) - self.order):\n",
    "                state = tuple(ids[i:i + self.order])\n",
    "                next_token = ids[i + self.order]\n",
    "                self.transitions[state].append(next_token)\n",
    "\n",
    "    def get_next_token(self, state, vocab_size):\n",
    "        state = tuple(state[-self.order:])\n",
    "        if state in self.transitions and self.transitions[state]:\n",
    "            return np.random.choice(self.transitions[state])\n",
    "        return np.random.randint(2, vocab_size)\n",
    "\n",
    "# Model components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        return self.proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Main model (matching the training code)\n",
    "class LyricsTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 256, d_model))\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generation function\n",
    "def generate_with_model(model, prompt, tokenizer, markov_chain, max_length=200, temperature=0.8, device='cpu'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, max_length=256).unsqueeze(0).to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    max_seq_length = 256\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(generated) > max_seq_length:\n",
    "                input_ids = torch.tensor(generated[-max_seq_length:]).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                input_ids = torch.tensor(generated).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[0, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Use Markov chain to guide sampling\n",
    "            markov_suggestion = markov_chain.get_next_token(generated, tokenizer.vocab_size)\n",
    "            markov_boost = torch.zeros_like(probs)\n",
    "            markov_boost[markov_suggestion] += 0.3\n",
    "            probs = F.softmax(probs + markov_boost, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            if next_token == tokenizer.pad_token_id:\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "\n",
    "    generated_lyrics = tokenizer.decode(torch.tensor(generated[len(tokenizer.encode(prompt)):]))\n",
    "    formatted_lyrics = format_lyrics(generated_lyrics)\n",
    "    return formatted_lyrics\n",
    "\n",
    "def format_lyrics(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 20:\n",
    "        return text\n",
    "\n",
    "    lines = []\n",
    "    line_length = 0\n",
    "    current_line = []\n",
    "    for word in words:\n",
    "        current_line.append(word)\n",
    "        line_length += 1\n",
    "        if line_length >= min(4, max(2, len(current_line)//2)) and word[-1] in '.,:;?!':\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "        elif line_length >= 8:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "\n",
    "    formatted_lyrics = []\n",
    "    formatted_lyrics.append(\"Verse 1:\")\n",
    "    verse_length = min(8, len(lines) // 2)\n",
    "    for i in range(verse_length):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Chorus:\")\n",
    "    chorus_start = verse_length\n",
    "    chorus_end = min(chorus_start + 4, len(lines))\n",
    "    for i in range(chorus_start, chorus_end):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    if chorus_end + 3 < len(lines):\n",
    "        formatted_lyrics.append(\"Verse 2:\")\n",
    "        verse2_end = min(chorus_end + 6, len(lines))\n",
    "        for i in range(chorus_end, verse2_end):\n",
    "            formatted_lyrics.append(lines[i])\n",
    "        formatted_lyrics.append(\"\")\n",
    "        formatted_lyrics.append(\"Chorus:\")\n",
    "        for i in range(chorus_start, chorus_end):\n",
    "            formatted_lyrics.append(lines[i])\n",
    "\n",
    "    return '\\n'.join(formatted_lyrics)\n",
    "\n",
    "def load_custom_lyrics_model(model_path=\"/content/final_lyrics_model.pth\"):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file {model_path} not found\")\n",
    "\n",
    "        # Add safe globals for secure loading\n",
    "        torch.serialization.add_safe_globals([LyricsTokenizer, MarkovChain])\n",
    "\n",
    "        # Load checkpoint\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "            print(\"Successfully loaded checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load checkpoint: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Extract tokenizer and Markov chain\n",
    "        tokenizer = checkpoint.get('tokenizer')\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not found in checkpoint\")\n",
    "        print(\"Loaded tokenizer from checkpoint\")\n",
    "\n",
    "        markov_chain = checkpoint.get('markov_chain')\n",
    "        if markov_chain is None:\n",
    "            raise ValueError(\"MarkovChain not found in checkpoint\")\n",
    "        print(\"Loaded MarkovChain from checkpoint\")\n",
    "\n",
    "        # Initialize model with correct vocab_size\n",
    "        model = LyricsTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            d_model=64,\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            d_ff=256\n",
    "        )\n",
    "\n",
    "        # Load model weights\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Loaded model weights from 'model_state_dict' key\")\n",
    "        else:\n",
    "            raise ValueError(\"Model state dictionary not found in checkpoint\")\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "        return model, tokenizer, markov_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_lyrics(theme, model, tokenizer, markov_chain):\n",
    "    try:\n",
    "        prompt = themes.get(theme.lower(), \"Write a beautiful song.\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        model = model.to(device)\n",
    "        lyrics = generate_with_model(model, prompt, tokenizer, markov_chain, device=device)\n",
    "        return lyrics\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during lyrics generation:\", e)\n",
    "        return None\n",
    "\n",
    "def text_to_speech(lyrics, output_file=\"lyrics_audio.mp3\"):\n",
    "    try:\n",
    "        tts = gTTS(text=lyrics, lang='en')\n",
    "        tts.save(output_file)\n",
    "        print(f\"Speech generated and saved as '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in text-to-speech conversion:\", e)\n",
    "\n",
    "def extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file):\n",
    "    try:\n",
    "        melody = AudioSegment.from_wav(melody_file)\n",
    "        lyrics = AudioSegment.from_mp3(lyrics_audio_file)\n",
    "        lyrics_duration = len(lyrics)\n",
    "        melody_duration = len(melody)\n",
    "        if melody_duration < lyrics_duration:\n",
    "            loop_count = (lyrics_duration // melody_duration) + 1\n",
    "            extended_melody = melody * loop_count\n",
    "            extended_melody = extended_melody[:lyrics_duration]\n",
    "        else:\n",
    "            extended_melody = melody[:lyrics_duration]\n",
    "        melody_volume = -6\n",
    "        extended_melody = extended_melody + melody_volume\n",
    "        final_output = extended_melody.overlay(lyrics)\n",
    "        final_output.export(output_file, format=\"mp3\")\n",
    "        print(f\"Final combined song saved as '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining audio: {e}\")\n",
    "\n",
    "def create_fallback_model_and_tokenizer():\n",
    "    print(\"\\nWARNING: Creating fallback model and tokenizer for testing purposes.\")\n",
    "    print(\"This will not generate coherent lyrics without proper training.\\n\")\n",
    "    tokenizer = LyricsTokenizer(vocab_size=10000)\n",
    "    sample_words = [\"the\", \"a\", \"and\", \"in\", \"of\", \"to\", \"is\", \"was\", \"it\",\n",
    "                    \"you\", \"i\", \"love\", \"nature\", \"sky\", \"tree\", \"river\", \"mountain\",\n",
    "                    \"friend\", \"hope\", \"dream\", \"sun\", \"moon\", \"star\", \"heart\", \"soul\",\n",
    "                    \"wind\", \"rain\", \"song\", \"music\", \"dance\", \"life\", \"time\", \"day\",\n",
    "                    \"night\", \"light\", \"dark\", \"beautiful\", \"happy\", \"sad\", \"free\"]\n",
    "    tokenizer.word_to_idx = {word: idx+2 for idx, word in enumerate(sample_words)}\n",
    "    tokenizer.word_to_idx[\"<pad>\"] = 0\n",
    "    tokenizer.word_to_idx[\"<unk>\"] = 1\n",
    "    tokenizer.idx_to_word = {idx: word for word, idx in tokenizer.word_to_idx.items()}\n",
    "    tokenizer.vocab_size = len(tokenizer.word_to_idx)\n",
    "    model = LyricsTransformer(vocab_size=tokenizer.vocab_size)\n",
    "    markov_chain = MarkovChain(order=2)\n",
    "    return model, tokenizer, markov_chain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theme = input(\"Enter a theme for your song (love, nature, friendship, hope, freedom): \").strip().lower()\n",
    "    if theme not in themes:\n",
    "        print(f\"Theme '{theme}' not recognized. Using 'love' as default.\")\n",
    "        theme = \"love\"\n",
    "\n",
    "    print(\"\\nLoading custom lyrics generator model...\\n\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    markov_chain = None\n",
    "    try:\n",
    "        model, tokenizer, markov_chain = load_custom_lyrics_model(\"/content/final_lyrics_model.pth\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error loading model: {e}\")\n",
    "        print(\"\\nWould you like to create a fallback model for testing purposes?\")\n",
    "        response = input(\"This won't generate good lyrics but will test the code flow (y/n): \").strip().lower()\n",
    "        if response == 'y' or response == 'yes':\n",
    "            model, tokenizer, markov_chain = create_fallback_model_and_tokenizer()\n",
    "        else:\n",
    "            print(\"Exiting program.\")\n",
    "            exit()\n",
    "\n",
    "    print(\"\\nGenerating Lyrics... Please wait.\\n\")\n",
    "    lyrics = generate_lyrics(theme, model, tokenizer, markov_chain)\n",
    "\n",
    "    if lyrics:\n",
    "        print(\"Generated Lyrics:\\n\")\n",
    "        print(lyrics)\n",
    "\n",
    "        lyrics_audio_file = \"lyrics_audio.mp3\"\n",
    "        text_to_speech(lyrics, lyrics_audio_file)\n",
    "\n",
    "        melody_file = input(\"\\nEnter path to melody file (default: generated_melody.wav): \").strip()\n",
    "        if not melody_file:\n",
    "            melody_file = \"/content/drive/MyDrive/GenAI/generated_melody.wav\"\n",
    "\n",
    "        if not os.path.exists(melody_file):\n",
    "            print(f\"Warning: Melody file '{melody_file}' not found. Please check the path.\")\n",
    "        else:\n",
    "            output_file = f\"{theme}_song.mp3\"\n",
    "            print(\"\\nCombining lyrics with melody...\\n\")\n",
    "            extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file)\n",
    "            print(f\"\\nSong generation complete! Your song is saved as '{output_file}'\")\n",
    "    else:\n",
    "        print(\"Failed to generate lyrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ks0Acy3fTDkJ",
    "outputId": "3afa33f8-24bc-40ea-82e3-9ed4f20329c6"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a theme for your song (love, nature, friendship, hope, freedom):  friendship\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading custom lyrics generator model...\n",
      "\n",
      "Successfully loaded checkpoint\n",
      "Loaded tokenizer from checkpoint\n",
      "Loaded MarkovChain from checkpoint\n",
      "Loaded model weights from 'model_state_dict' key\n",
      "Model loaded successfully from final_lyrics_model.pth\n",
      "\n",
      "Generating Lyrics... Please wait.\n",
      "\n",
      "Using device: cpu\n",
      "Encoded prompt: ['write', 'a', 'song', 'about', 'the', 'power', 'and', 'importance', 'of', 'friendship'] -> [1, 1, 32, 1, 1, 1, 1, 1, 1, 1]... (length: 256)\n",
      "Encoded prompt: ['write', 'a', 'song', 'about', 'the', 'power', 'and', 'importance', 'of', 'friendship'] -> [1, 1, 32, 1, 1, 1, 1, 1, 1, 1]... (length: 256)\n",
      "Generated Lyrics:\n",
      "\n",
      "Verse 1:\n",
      "light eyes darling baby shine forever hope\n",
      "come life sky always fear fly life\n",
      "touch baby baby run know star eyes\n",
      "back run see song stay rhythm heart\n",
      "mind sing light hold moon rhyme baby\n",
      "darling time always light spirit dark tears\n",
      "hands know mind dreams always eyes home\n",
      "feel shadow sky gone dreams eyes free\n",
      "\n",
      "Chorus:\n",
      "light road kiss see road life apart\n",
      "joy joy hands never hold know hold\n",
      "beat dance time shine soul mind world\n",
      "tears light fire kiss mind apart night\n",
      "\n",
      "Verse 2:\n",
      "melody always mind home way smile way\n",
      "moon smile shine star dark hands heart\n",
      "way fear go love love burn rise\n",
      "joy rise joy smile tears break fire\n",
      "sky kiss free heart touch shadow time\n",
      "hope darling way fall rhyme touch smile\n",
      "darling kiss song fall hold gone apart\n",
      "joy kiss know hold fire dance hands\n",
      "\n",
      "Bridge:\n",
      "smile hold dreams leave together dreams spirit\n",
      "light shine fear joy see always kiss\n",
      "moon rhyme melody light hope dark cry\n",
      "touch free fire darling way smile world\n",
      "\n",
      "Verse 3:\n",
      "time rise day gone night dance free\n",
      "life sun home forever hold sun free\n",
      "never world rise know see never always\n",
      "fly kiss dance sing fire hold star\n",
      "mind back fall never star stay heart\n",
      "sing melody voice see break hands dark\n",
      "world hope darling gone kiss soul always\n",
      "beat voice stay mind fall sun see\n",
      "\n",
      "Chorus:\n",
      "light road kiss see road life apart\n",
      "joy joy hands never hold know hold\n",
      "beat dance time shine soul mind world\n",
      "tears light fire kiss mind apart night\n",
      "\n",
      "Outro:\n",
      "way pain apart sun come light always\n",
      "tears stay tears run time sing shine\n",
      "rhythm kiss see stay world smile memory\n",
      "Speech generated and saved as 'lyrics_audio.mp3'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter path to melody file (default: generated_melody.wav):  \"D:\\PROJECT\\GenAI_prj\\FINAL\\generated_melody.wav\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Melody file '\"D:\\PROJECT\\GenAI_prj\\FINAL\\generated_melody.wav\"' not found. Please check the path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the themes and the prompt for each theme\n",
    "themes = {\n",
    "    \"love\": \"Write a beautiful song about love and relationships.\",\n",
    "    \"nature\": \"Write a song about the beauty of nature and the environment.\",\n",
    "    \"friendship\": \"Write a song about the power and importance of friendship.\",\n",
    "    \"hope\": \"Write a song about hope and positivity for the future.\",\n",
    "    \"freedom\": \"Write a song about freedom and independence.\"\n",
    "}\n",
    "\n",
    "# Lyrics-specific vocabulary with theme-specific words\n",
    "COMMON_LYRICS_WORDS = [\n",
    "    '<pad>', '<unk>', 'love', 'heart', 'dream', 'night', 'day', 'life', 'time', 'way',\n",
    "    'feel', 'know', 'go', 'see', 'world', 'soul', 'sky', 'star', 'moon', 'sun',\n",
    "    'baby', 'darling', 'forever', 'always', 'never', 'together', 'apart', 'home', 'road', 'fire',\n",
    "    'dance', 'sing', 'song', 'melody', 'rhyme', 'beat', 'rhythm', 'free', 'run', 'fly',\n",
    "    'hold', 'touch', 'kiss', 'smile', 'cry', 'tears', 'pain', 'joy', 'hope', 'fear',\n",
    "    'light', 'dark', 'shadow', 'shine', 'burn', 'break', 'fall', 'rise', 'stay', 'leave',\n",
    "    'come', 'gone', 'back', 'memory', 'dreams', 'eyes', 'hands', 'voice', 'mind', 'spirit',\n",
    "    # Love\n",
    "    'passion', 'devotion', 'eternal', 'cherish', 'adore', 'sweet', 'romance', 'embrace', 'lover', 'dear',\n",
    "    # Nature\n",
    "    'forest', 'river', 'mountain', 'ocean', 'wind', 'tree', 'flower', 'earth', 'valley', 'meadow',\n",
    "    'stream', 'breeze', 'horizon', 'dawn', 'twilight', 'rain', 'mist', 'lake', 'pine', 'bloom',\n",
    "    # Friendship\n",
    "    'friend', 'bond', 'trust', 'loyal', 'share', 'laughter', 'support', 'care', 'companion', 'unity',\n",
    "    # Hope\n",
    "    'future', 'promise', 'vision', 'aspire', 'uplift', 'believe', 'shine', 'tomorrow', 'wish', 'dreamer',\n",
    "    # Freedom\n",
    "    'liberty', 'wings', 'open', 'skyward', 'unbound', 'journey', 'release', 'soar', 'freebird', 'escape'\n",
    "]\n",
    "\n",
    "# Theme-specific word lists for boosting\n",
    "THEME_WORDS = {\n",
    "    \"love\": [\n",
    "        'love', 'heart', 'kiss', 'passion', 'devotion', 'eternal', 'cherish', 'adore', 'sweet', 'romance',\n",
    "        'embrace', 'lover', 'dear', 'baby', 'darling', 'forever', 'always', 'together', 'soul', 'dream'\n",
    "    ],\n",
    "    \"nature\": [\n",
    "        'forest', 'river', 'mountain', 'ocean', 'wind', 'tree', 'flower', 'earth', 'valley', 'meadow',\n",
    "        'stream', 'breeze', 'horizon', 'dawn', 'twilight', 'rain', 'mist', 'lake', 'pine', 'bloom'\n",
    "    ],\n",
    "    \"friendship\": [\n",
    "        'friend', 'bond', 'trust', 'loyal', 'share', 'laughter', 'support', 'care', 'companion', 'unity'\n",
    "    ],\n",
    "    \"hope\": [\n",
    "        'hope', 'future', 'promise', 'vision', 'aspire', 'uplift', 'believe', 'shine', 'tomorrow', 'dreamer'\n",
    "    ],\n",
    "    \"freedom\": [\n",
    "        'free', 'liberty', 'wings', 'open', 'skyward', 'unbound', 'journey', 'release', 'soar', 'freebird'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Custom Tokenizer\n",
    "class LyricsTokenizer:\n",
    "    def __init__(self, vocab_size=15000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(COMMON_LYRICS_WORDS)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        words = []\n",
    "        for text in tqdm(texts, desc=\"Building vocabulary\"):\n",
    "            words.extend(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        additional_words = [word for word, _ in word_counts.most_common(self.vocab_size - len(COMMON_LYRICS_WORDS))]\n",
    "        for word in additional_words:\n",
    "            if word not in self.word_to_idx and len(self.word_to_idx) < self.vocab_size:\n",
    "                self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = word\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text, max_length=256):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        ids = [self.word_to_idx.get(word, self.unk_token_id) for word in words]\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_length]\n",
    "        print(f\"Encoded prompt: {words} -> {ids[:10]}... (length: {len(ids)})\")\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = [self.idx_to_word.get(id.item(), '<unk>') for id in ids if id != self.pad_token_id]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Markov Chain\n",
    "class MarkovChain:\n",
    "    def __init__(self, order=3):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(list)\n",
    "\n",
    "    def fit(self, texts, tokenizer):\n",
    "        for text in tqdm(texts, desc=\"Building Markov chain\"):\n",
    "            ids = tokenizer.encode(text).tolist()\n",
    "            for i in range(len(ids) - self.order):\n",
    "                state = tuple(ids[i:i + self.order])\n",
    "                next_token = ids[i + self.order]\n",
    "                self.transitions[state].append(next_token)\n",
    "\n",
    "    def get_next_token(self, state, vocab_size):\n",
    "        state = tuple(state[-self.order:])\n",
    "        if state in self.transitions and self.transitions[state]:\n",
    "            return np.random.choice(self.transitions[state])\n",
    "        return np.random.randint(2, vocab_size)\n",
    "\n",
    "# Model components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        return self.proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Main model\n",
    "class LyricsTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 256, d_model))  # Match checkpoint\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generation function with theme-specific boosting\n",
    "def generate_with_model(model, prompt, tokenizer, markov_chain, max_length=600, temperature=0.85, device='cpu', theme='love'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, max_length=256).unsqueeze(0).to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    max_seq_length = 256\n",
    "\n",
    "    # Get indices of theme-specific words\n",
    "    theme_word_ids = [tokenizer.word_to_idx[word] for word in THEME_WORDS.get(theme, []) if word in tokenizer.word_to_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(generated) > max_seq_length:\n",
    "                input_ids = torch.tensor(generated[-max_seq_length:]).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                input_ids = torch.tensor(generated).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[0, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Boost theme-specific words and Markov suggestions\n",
    "            markov_suggestion = markov_chain.get_next_token(generated, tokenizer.vocab_size)\n",
    "            markov_boost = torch.zeros_like(probs)\n",
    "            markov_boost[markov_suggestion] += 0.7\n",
    "            for theme_id in theme_word_ids:\n",
    "                markov_boost[theme_id] += 0.6\n",
    "            probs = F.softmax(probs + markov_boost, dim=-1)\n",
    "            probs[tokenizer.unk_token_id] *= 0.01  # Penalize <unk> tokens\n",
    "            probs[tokenizer.pad_token_id] *= 0.01  # Avoid early termination\n",
    "\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            generated.append(next_token)\n",
    "            if len(generated) >= max_length:\n",
    "                break\n",
    "\n",
    "    generated_lyrics = tokenizer.decode(torch.tensor(generated[len(tokenizer.encode(prompt)):]))\n",
    "    formatted_lyrics = format_lyrics(generated_lyrics, theme)\n",
    "    return formatted_lyrics\n",
    "\n",
    "def format_lyrics(text, theme):\n",
    "    words = text.split()\n",
    "    if len(words) < 40:\n",
    "        words.extend(THEME_WORDS.get(theme, LOVE_WORDS) * 3)\n",
    "        print(f\"Warning: Generated text too short ({len(words)} words). Padded with {theme}-themed words.\")\n",
    "\n",
    "    lines = []\n",
    "    line_length = 0\n",
    "    current_line = []\n",
    "    for word in words:\n",
    "        current_line.append(word)\n",
    "        line_length += 1\n",
    "        if line_length >= min(5, max(3, len(current_line)//2)) and word[-1] in '.,:;?!':\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "        elif line_length >= 7:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "\n",
    "    formatted_lyrics = []\n",
    "    total_lines = len(lines)\n",
    "    formatted_lyrics.append(\"Verse 1:\")\n",
    "    verse_length = min(8, total_lines // 4)\n",
    "    for i in range(min(verse_length, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Chorus:\")\n",
    "    chorus_start = verse_length\n",
    "    chorus_end = min(chorus_start + 4, total_lines)\n",
    "    for i in range(chorus_start, min(chorus_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Verse 2:\")\n",
    "    verse2_start = chorus_end\n",
    "    verse2_end = min(verse2_start + 8, total_lines)\n",
    "    for i in range(verse2_start, min(verse2_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Bridge:\")\n",
    "    bridge_start = verse2_end\n",
    "    bridge_end = min(bridge_start + 4, total_lines)\n",
    "    for i in range(bridge_start, min(bridge_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Verse 3:\")\n",
    "    verse3_start = bridge_end\n",
    "    verse3_end = min(verse3_start + 8, total_lines)\n",
    "    for i in range(verse3_start, min(verse3_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Chorus:\")\n",
    "    for i in range(chorus_start, min(chorus_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Outro:\")\n",
    "    outro_start = verse3_end\n",
    "    outro_end = min(outro_start + 3, total_lines)\n",
    "    for i in range(outro_start, min(outro_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "\n",
    "    return '\\n'.join(formatted_lyrics)\n",
    "\n",
    "def load_custom_lyrics_model(model_path=\"/content/final_lyrics_model.pth\"):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file {model_path} not found\")\n",
    "\n",
    "        torch.serialization.add_safe_globals([LyricsTokenizer, MarkovChain])\n",
    "\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        print(\"Successfully loaded checkpoint\")\n",
    "\n",
    "        tokenizer = checkpoint.get('tokenizer')\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not found in checkpoint\")\n",
    "        print(\"Loaded tokenizer from checkpoint\")\n",
    "\n",
    "        markov_chain = checkpoint.get('markov_chain')\n",
    "        if markov_chain is None:\n",
    "            raise ValueError(\"MarkovChain not found in checkpoint\")\n",
    "        print(\"Loaded MarkovChain from checkpoint\")\n",
    "\n",
    "        model = LyricsTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            d_model=64,\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            d_ff=256\n",
    "        )\n",
    "\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Loaded model weights from 'model_state_dict' key\")\n",
    "        else:\n",
    "            raise ValueError(\"Model state dictionary not found in checkpoint\")\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "        return model, tokenizer, markov_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_lyrics(theme, model, tokenizer, markov_chain):\n",
    "    try:\n",
    "        prompt = themes.get(theme.lower(), \"Write a beautiful song.\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        model = model.to(device)\n",
    "        lyrics = generate_with_model(model, prompt, tokenizer, markov_chain, device=device, theme=theme)\n",
    "        return lyrics\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during lyrics generation:\", e)\n",
    "        return None\n",
    "\n",
    "def text_to_speech(lyrics, output_file=\"lyrics_audio.mp3\"):\n",
    "    try:\n",
    "        tts = gTTS(text=lyrics, lang='en')\n",
    "        tts.save(output_file)\n",
    "        print(f\"Speech generated and saved as '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in text-to-speech conversion:\", e)\n",
    "\n",
    "def extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file):\n",
    "    try:\n",
    "        melody = AudioSegment.from_wav(melody_file)\n",
    "        lyrics = AudioSegment.from_mp3(lyrics_audio_file)\n",
    "        lyrics_duration = len(lyrics)\n",
    "        melody_duration = len(melody)\n",
    "        if melody_duration < lyrics_duration:\n",
    "            loop_count = (lyrics_duration // melody_duration) + 1\n",
    "            extended_melody = melody * loop_count\n",
    "            extended_melody = extended_melody[:lyrics_duration]\n",
    "        else:\n",
    "            extended_melody = melody[:lyrics_duration]\n",
    "        melody_volume = -6\n",
    "        extended_melody = extended_melody + melody_volume\n",
    "        final_output = extended_melody.overlay(lyrics)\n",
    "        final_output.export(output_file, format=\"mp3\")\n",
    "        print(f\"Final combined song saved as '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining audio: {e}\")\n",
    "\n",
    "def create_fallback_model_and_tokenizer(theme=\"love\"):\n",
    "    print(\"\\nWARNING: Creating fallback model and tokenizer for testing purposes.\")\n",
    "    print(\"This will not generate coherent lyrics without proper training.\\n\")\n",
    "    tokenizer = LyricsTokenizer(vocab_size=15000)\n",
    "    sample_words = [\n",
    "        \"the\", \"a\", \"and\", \"in\", \"of\", \"to\", \"is\", \"was\", \"it\", \"you\", \"i\",\n",
    "        \"life\", \"time\", \"way\", \"feel\", \"know\", \"go\", \"see\", \"world\", \"soul\", \"sky\"\n",
    "    ] + THEME_WORDS.get(theme, THEME_WORDS[\"love\"])\n",
    "    tokenizer.word_to_idx = {word: idx+2 for idx, word in enumerate(sample_words)}\n",
    "    tokenizer.word_to_idx[\"<pad>\"] = 0\n",
    "    tokenizer.word_to_idx[\"<unk>\"] = 1\n",
    "    tokenizer.idx_to_word = {idx: word for word, idx in tokenizer.word_to_idx.items()}\n",
    "    tokenizer.vocab_size = len(tokenizer.word_to_idx)\n",
    "    model = LyricsTransformer(vocab_size=tokenizer.vocab_size)\n",
    "    markov_chain = MarkovChain(order=3)\n",
    "    return model, tokenizer, markov_chain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theme = input(\"Enter a theme for your song (love, nature, friendship, hope, freedom): \").strip().lower()\n",
    "    if theme not in themes:\n",
    "        print(f\"Theme '{theme}' not recognized. Using 'love' as default.\")\n",
    "        theme = \"love\"\n",
    "\n",
    "    print(\"\\nLoading custom lyrics generator model...\\n\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    markov_chain = None\n",
    "    try:\n",
    "        model, tokenizer, markov_chain = load_custom_lyrics_model(\"final_lyrics_model.pth\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error loading model: {e}\")\n",
    "        print(\"\\nWould you like to create a fallback model for testing purposes?\")\n",
    "        response = input(\"This won't generate good lyrics but will test the code flow (y/n): \").strip().lower()\n",
    "        if response == 'y' or response == 'yes':\n",
    "            model, tokenizer, markov_chain = create_fallback_model_and_tokenizer(theme)\n",
    "        else:\n",
    "            print(\"Exiting program.\")\n",
    "            exit()\n",
    "\n",
    "    print(\"\\nGenerating Lyrics... Please wait.\\n\")\n",
    "    lyrics = generate_lyrics(theme, model, tokenizer, markov_chain)\n",
    "\n",
    "    if lyrics:\n",
    "        print(\"Generated Lyrics:\\n\")\n",
    "        print(lyrics)\n",
    "\n",
    "        lyrics_audio_file = \"lyrics_audio.mp3\"\n",
    "        text_to_speech(lyrics, lyrics_audio_file)\n",
    "\n",
    "        melody_file = input(\"\\nEnter path to melody file (default: generated_melody.wav): \").strip()\n",
    "        if not melody_file:\n",
    "            melody_file = \"/content/drive/MyDrive/GenAI/generated_melody.wav\"\n",
    "\n",
    "        if not os.path.exists(melody_file):\n",
    "            print(f\"Warning: Melody file '{melody_file}' not found. Please check the path.\")\n",
    "        else:\n",
    "            output_file = f\"{theme}_song.mp3\"\n",
    "            print(\"\\nCombining lyrics with melody...\\n\")\n",
    "            extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file)\n",
    "            print(f\"\\nSong generation complete! Your song is saved as '{output_file}'\")\n",
    "    else:\n",
    "        print(\"Failed to generate lyrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrNd-Gs3TFkc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\PROJECT\\GenAI_prj\\FINAL\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a theme for your song (love, nature, friendship, hope, freedom):  love\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading custom lyrics generator model...\n",
      "\n",
      "Error loading model: Model file final_lyrics_model.pth not found\n",
      "\n",
      "Fatal error loading model: Model file final_lyrics_model.pth not found\n",
      "\n",
      "Would you like to create a fallback model for testing purposes?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Define the themes and the prompt for each theme\n",
    "themes = {\n",
    "    \"love\": \"Write a beautiful song about love and relationships.\",\n",
    "    \"nature\": \"Write a song about the beauty of nature and the environment.\",\n",
    "    \"friendship\": \"Write a song about the power and importance of friendship.\",\n",
    "    \"hope\": \"Write a song about hope and positivity for the future.\",\n",
    "    \"freedom\": \"Write a song about freedom and independence.\"\n",
    "}\n",
    "\n",
    "# Lyrics-specific vocabulary with theme-specific words\n",
    "COMMON_LYRICS_WORDS = [\n",
    "    '<pad>', '<unk>', 'love', 'heart', 'dream', 'night', 'day', 'life', 'time', 'way',\n",
    "    'feel', 'know', 'go', 'see', 'world', 'soul', 'sky', 'star', 'moon', 'sun',\n",
    "    'baby', 'darling', 'forever', 'always', 'never', 'together', 'apart', 'home', 'road', 'fire',\n",
    "    'dance', 'sing', 'song', 'melody', 'rhyme', 'beat', 'rhythm', 'free', 'run', 'fly',\n",
    "    'hold', 'touch', 'kiss', 'smile', 'cry', 'tears', 'pain', 'joy', 'hope', 'fear',\n",
    "    'light', 'dark', 'shadow', 'shine', 'burn', 'break', 'fall', 'rise', 'stay', 'leave',\n",
    "    'come', 'gone', 'back', 'memory', 'dreams', 'eyes', 'hands', 'voice', 'mind', 'spirit',\n",
    "    'passion', 'devotion', 'eternal', 'cherish', 'adore', 'sweet', 'romance', 'embrace', 'lover', 'dear',\n",
    "    'forest', 'river', 'mountain', 'ocean', 'wind', 'tree', 'flower', 'earth', 'valley', 'meadow',\n",
    "    'stream', 'breeze', 'horizon', 'dawn', 'twilight', 'rain', 'mist', 'lake', 'pine', 'bloom',\n",
    "    'friend', 'bond', 'trust', 'loyal', 'share', 'laughter', 'support', 'care', 'companion', 'unity',\n",
    "    'future', 'promise', 'vision', 'aspire', 'uplift', 'believe', 'shine', 'tomorrow', 'wish', 'dreamer',\n",
    "    'liberty', 'wings', 'open', 'skyward', 'unbound', 'journey', 'release', 'soar', 'freebird', 'escape',\n",
    "    'write', 'beautiful', 'about', 'and', 'relationships', 'environment', 'power', 'importance', 'positivity', 'independence'\n",
    "]\n",
    "\n",
    "# Theme-specific word lists for boosting\n",
    "THEME_WORDS = {\n",
    "    \"love\": [\n",
    "        'love', 'heart', 'kiss', 'passion', 'devotion', 'eternal', 'cherish', 'adore', 'sweet', 'romance',\n",
    "        'embrace', 'lover', 'dear', 'baby', 'darling', 'forever', 'always', 'together', 'soul', 'dream'\n",
    "    ],\n",
    "    \"nature\": [\n",
    "        'forest', 'river', 'mountain', 'ocean', 'wind', 'tree', 'flower', 'earth', 'valley', 'meadow',\n",
    "        'stream', 'breeze', 'horizon', 'dawn', 'twilight', 'rain', 'mist', 'lake', 'pine', 'bloom'\n",
    "    ],\n",
    "    \"friendship\": [\n",
    "        'friend', 'bond', 'trust', 'loyal', 'share', 'laughter', 'support', 'care', 'companion', 'unity'\n",
    "    ],\n",
    "    \"hope\": [\n",
    "        'hope', 'future', 'promise', 'vision', 'aspire', 'uplift', 'believe', 'shine', 'tomorrow', 'dreamer'\n",
    "    ],\n",
    "    \"freedom\": [\n",
    "        'free', 'liberty', 'wings', 'open', 'skyward', 'unbound', 'journey', 'release', 'soar', 'freebird'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Custom Tokenizer\n",
    "class LyricsTokenizer:\n",
    "    def __init__(self, vocab_size=15000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(COMMON_LYRICS_WORDS)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        words = []\n",
    "        for text in tqdm(texts, desc=\"Building vocabulary\"):\n",
    "            words.extend(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        additional_words = [word for word, _ in word_counts.most_common(self.vocab_size - len(COMMON_LYRICS_WORDS))]\n",
    "        for word in additional_words:\n",
    "            if word not in self.word_to_idx and len(self.word_to_idx) < self.vocab_size:\n",
    "                self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = word\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text, max_length=256):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        ids = [self.word_to_idx.get(word, self.unk_token_id) for word in words]\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_length]\n",
    "        print(f\"Encoded prompt: {words} -> {ids[:10]}... (length: {len(ids)})\")\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = [self.idx_to_word.get(id.item(), '<unk>') for id in ids if id != self.pad_token_id]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Markov Chain\n",
    "class MarkovChain:\n",
    "    def __init__(self, order=3):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(list)\n",
    "\n",
    "    def fit(self, texts, tokenizer):\n",
    "        for text in tqdm(texts, desc=\"Building Markov chain\"):\n",
    "            ids = tokenizer.encode(text).tolist()\n",
    "            for i in range(len(ids) - self.order):\n",
    "                state = tuple(ids[i:i + self.order])\n",
    "                next_token = ids[i + self.order]\n",
    "                self.transitions[state].append(next_token)\n",
    "\n",
    "    def get_next_token(self, state, vocab_size):\n",
    "        state = tuple(state[-self.order:])\n",
    "        if state in self.transitions and self.transitions[state]:\n",
    "            return np.random.choice(self.transitions[state])\n",
    "        return np.random.randint(2, vocab_size)\n",
    "\n",
    "# Model components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        return self.proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Main model\n",
    "class LyricsTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 256, d_model))  # Match checkpoint\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generation function with theme-specific boosting\n",
    "def generate_with_model(model, prompt, tokenizer, markov_chain, max_length=600, temperature=0.85, device='cpu', theme='love'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, max_length=256).unsqueeze(0).to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "    max_seq_length = 256\n",
    "\n",
    "    # Get indices of theme-specific words\n",
    "    theme_word_ids = [tokenizer.word_to_idx[word] for word in THEME_WORDS.get(theme, []) if word in tokenizer.word_to_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(generated) > max_seq_length:\n",
    "                input_ids = torch.tensor(generated[-max_seq_length:]).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                input_ids = torch.tensor(generated).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[0, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Boost theme-specific words and Markov suggestions\n",
    "            markov_suggestion = markov_chain.get_next_token(generated, tokenizer.vocab_size)\n",
    "            markov_boost = torch.zeros_like(probs)\n",
    "            markov_boost[markov_suggestion] += 0.7\n",
    "            for theme_id in theme_word_ids:\n",
    "                markov_boost[theme_id] += 0.6\n",
    "            probs = F.softmax(probs + markov_boost, dim=-1)\n",
    "            probs[tokenizer.unk_token_id] *= 0.01  # Penalize <unk> tokens\n",
    "            probs[tokenizer.pad_token_id] *= 0.01  # Avoid early termination\n",
    "\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            generated.append(next_token)\n",
    "            if len(generated) >= max_length:\n",
    "                break\n",
    "\n",
    "    generated_lyrics = tokenizer.decode(torch.tensor(generated[len(tokenizer.encode(prompt)):]))\n",
    "    formatted_lyrics = format_lyrics(generated_lyrics, theme)\n",
    "    return formatted_lyrics\n",
    "\n",
    "def format_lyrics(text, theme):\n",
    "    words = text.split()\n",
    "    if len(words) < 40:\n",
    "        words.extend(THEME_WORDS.get(theme, THEME_WORDS[\"love\"]) * 3)\n",
    "        print(f\"Warning: Generated text too short ({len(words)} words). Padded with {theme}-themed words.\")\n",
    "\n",
    "    lines = []\n",
    "    line_length = 0\n",
    "    current_line = []\n",
    "    for word in words:\n",
    "        current_line.append(word)\n",
    "        line_length += 1\n",
    "        if line_length >= min(5, max(3, len(current_line)//2)) and word[-1] in '.,:;?!':\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "        elif line_length >= 7:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = []\n",
    "            line_length = 0\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "\n",
    "    formatted_lyrics = []\n",
    "    total_lines = len(lines)\n",
    "    formatted_lyrics.append(\"Verse 1:\")\n",
    "    verse_length = min(8, total_lines // 4)\n",
    "    for i in range(min(verse_length, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Chorus:\")\n",
    "    chorus_start = verse_length\n",
    "    chorus_end = min(chorus_start + 4, total_lines)\n",
    "    for i in range(chorus_start, min(chorus_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Verse 2:\")\n",
    "    verse2_start = chorus_end\n",
    "    verse2_end = min(verse2_start + 8, total_lines)\n",
    "    for i in range(verse2_start, min(verse2_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Bridge:\")\n",
    "    bridge_start = verse2_end\n",
    "    bridge_end = min(bridge_start + 4, total_lines)\n",
    "    for i in range(bridge_start, min(bridge_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Verse 3:\")\n",
    "    verse3_start = bridge_end\n",
    "    verse3_end = min(verse3_start + 8, total_lines)\n",
    "    for i in range(verse3_start, min(verse3_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Chorus:\")\n",
    "    for i in range(chorus_start, min(chorus_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "    formatted_lyrics.append(\"\")\n",
    "\n",
    "    formatted_lyrics.append(\"Outro:\")\n",
    "    outro_start = verse3_end\n",
    "    outro_end = min(outro_start + 3, total_lines)\n",
    "    for i in range(outro_start, min(outro_end, total_lines)):\n",
    "        formatted_lyrics.append(lines[i])\n",
    "\n",
    "    return '\\n'.join(formatted_lyrics)\n",
    "\n",
    "def load_custom_lyrics_model(model_path=\"final_lyrics_model.pth\"):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file {model_path} not found\")\n",
    "\n",
    "        torch.serialization.add_safe_globals([LyricsTokenizer, MarkovChain])\n",
    "\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        print(\"Successfully loaded checkpoint\")\n",
    "\n",
    "        tokenizer = checkpoint.get('tokenizer')\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not found in checkpoint\")\n",
    "        print(\"Loaded tokenizer from checkpoint\")\n",
    "\n",
    "        markov_chain = checkpoint.get('markov_chain')\n",
    "        if markov_chain is None:\n",
    "            raise ValueError(\"MarkovChain not found in checkpoint\")\n",
    "        print(\"Loaded MarkovChain from checkpoint\")\n",
    "\n",
    "        model = LyricsTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            d_model=64,\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            d_ff=256\n",
    "        )\n",
    "\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Loaded model weights from 'model_state_dict' key\")\n",
    "        else:\n",
    "            raise ValueError(\"Model state dictionary not found in checkpoint\")\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "        return model, tokenizer, markov_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_lyrics(theme, model, tokenizer, markov_chain):\n",
    "    try:\n",
    "        prompt = themes.get(theme.lower(), \"Write a beautiful song.\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        model = model.to(device)\n",
    "        lyrics = generate_with_model(model, prompt, tokenizer, markov_chain, device=device, theme=theme)\n",
    "        return lyrics\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during lyrics generation:\", e)\n",
    "        return None\n",
    "\n",
    "def text_to_speech(lyrics, output_file=\"lyrics_audio.mp3\"):\n",
    "    try:\n",
    "        # Ensure output_file is in the current working directory\n",
    "        output_file = os.path.join(os.getcwd(), output_file)\n",
    "        tts = gTTS(text=lyrics, lang='en')\n",
    "        tts.save(output_file)\n",
    "        print(f\"Speech generated and saved as '{output_file}'\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error in text-to-speech conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "def extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file):\n",
    "    try:\n",
    "        if not os.path.exists(melody_file):\n",
    "            raise FileNotFoundError(f\"Melody file '{melody_file}' not found\")\n",
    "        if not os.path.exists(lyrics_audio_file):\n",
    "            raise FileNotFoundError(f\"Lyrics audio file '{lyrics_audio_file}' not found\")\n",
    "\n",
    "        # Ensure output_file is in the current working directory\n",
    "        output_file = os.path.join(os.getcwd(), output_file)\n",
    "        melody = AudioSegment.from_wav(melody_file)\n",
    "        lyrics = AudioSegment.from_mp3(lyrics_audio_file)\n",
    "        lyrics_duration = len(lyrics)\n",
    "        melody_duration = len(melody)\n",
    "        if melody_duration < lyrics_duration:\n",
    "            loop_count = (lyrics_duration // melody_duration) + 1\n",
    "            extended_melody = melody * loop_count\n",
    "            extended_melody = extended_melody[:lyrics_duration]\n",
    "        else:\n",
    "            extended_melody = melody[:lyrics_duration]\n",
    "        melody_volume = -6\n",
    "        extended_melody = extended_melody + melody_volume\n",
    "        final_output = extended_melody.overlay(lyrics)\n",
    "        final_output.export(output_file, format=\"mp3\")\n",
    "        print(f\"Final combined song saved as '{output_file}'\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_fallback_model_and_tokenizer(theme=\"love\"):\n",
    "    print(\"\\nWARNING: Creating fallback model and tokenizer for testing purposes.\")\n",
    "    print(\"This will not generate coherent lyrics without proper training.\\n\")\n",
    "    tokenizer = LyricsTokenizer(vocab_size=15000)\n",
    "    sample_words = [\n",
    "        \"the\", \"a\", \"and\", \"in\", \"of\", \"to\", \"is\", \"was\", \"it\", \"you\", \"i\",\n",
    "        \"life\", \"time\", \"way\", \"feel\", \"know\", \"go\", \"see\", \"world\", \"soul\", \"sky\",\n",
    "        \"write\", \"beautiful\", \"about\", \"relationships\", \"environment\", \"power\", \"importance\"\n",
    "    ] + THEME_WORDS.get(theme, THEME_WORDS[\"love\"])\n",
    "    tokenizer.word_to_idx = {word: idx+2 for idx, word in enumerate(sample_words)}\n",
    "    tokenizer.word_to_idx[\"<pad>\"] = 0\n",
    "    tokenizer.word_to_idx[\"<unk>\"] = 1\n",
    "    tokenizer.idx_to_word = {word: idx for idx, word in tokenizer.word_to_idx.items()}\n",
    "    tokenizer.vocab_size = len(tokenizer.word_to_idx)\n",
    "    model = LyricsTransformer(vocab_size=tokenizer.vocab_size)\n",
    "    markov_chain = MarkovChain(order=3)\n",
    "    return model, tokenizer, markov_chain\n",
    "\n",
    "def correct_theme(theme):\n",
    "    \"\"\"Correct potential typos in theme input using fuzzy matching.\"\"\"\n",
    "    theme = theme.lower().strip()\n",
    "    if theme in themes:\n",
    "        return theme\n",
    "    possible_matches = get_close_matches(theme, themes.keys(), n=1, cutoff=0.6)\n",
    "    if possible_matches:\n",
    "        corrected_theme = possible_matches[0]\n",
    "        print(f\"Corrected theme '{theme}' to '{corrected_theme}'\")\n",
    "        return corrected_theme\n",
    "    print(f\"Theme '{theme}' not recognized. Using 'love' as default.\")\n",
    "    return \"love\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print current working directory\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "    theme = input(\"Enter a theme for your song (love, nature, friendship, hope, freedom): \").strip()\n",
    "    theme = correct_theme(theme)  # Correct typos\n",
    "\n",
    "    print(\"\\nLoading custom lyrics generator model...\\n\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    markov_chain = None\n",
    "    try:\n",
    "        model, tokenizer, markov_chain = load_custom_lyrics_model(\"final_lyrics_model.pth\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error loading model: {e}\")\n",
    "        print(\"\\nWould you like to create a fallback model for testing purposes?\")\n",
    "        response = input(\"This won't generate good lyrics but will test the code flow (y/n): \").strip().lower()\n",
    "        if response == 'y' or response == 'yes':\n",
    "            model, tokenizer, markov_chain = create_fallback_model_and_tokenizer(theme)\n",
    "        else:\n",
    "            print(\"Exiting program.\")\n",
    "            exit()\n",
    "\n",
    "    print(\"\\nGenerating Lyrics... Please wait.\\n\")\n",
    "    lyrics = generate_lyrics(theme, model, tokenizer, markov_chain)\n",
    "\n",
    "    if lyrics:\n",
    "        print(\"Generated Lyrics:\\n\")\n",
    "        print(lyrics)\n",
    "\n",
    "        lyrics_audio_file = \"lyrics_audio.mp3\"\n",
    "        lyrics_audio_file = text_to_speech(lyrics, lyrics_audio_file)\n",
    "        if not lyrics_audio_file:\n",
    "            print(\"Failed to generate lyrics audio. Skipping melody combination.\")\n",
    "        else:\n",
    "            melody_file = input(\"\\nEnter path to melody file (default: generated_melody.wav): \").strip()\n",
    "            if not melody_file:\n",
    "                melody_file = \"generated_melody.wav\"\n",
    "\n",
    "            output_file = f\"{theme}_song.mp3\"\n",
    "            if os.path.exists(melody_file):\n",
    "                print(\"\\nCombining lyrics with melody...\\n\")\n",
    "                result = extend_melody_to_match_lyrics(lyrics_audio_file, melody_file, output_file)\n",
    "                if result:\n",
    "                    print(f\"\\nSong generation complete! Your song is saved as '{result}'\")\n",
    "                else:\n",
    "                    print(\"\\nFailed to combine lyrics with melody. Check the error above.\")\n",
    "            else:\n",
    "                print(f\"Melody file '{melody_file}' not found. Please provide a valid path.\")\n",
    "                print(f\"Lyrics audio is saved as '{lyrics_audio_file}'\")\n",
    "    else:\n",
    "        print(\"Failed to generate lyrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "012d4d9e6dd7490cbda4184eb0f48d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "04e3389a7e054d8ca8aaf0f583e44668": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0bb3d91de90449bba05a6f4c75dc2d43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0df43236fabd449385bcc5799a3e2ac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e308b60314843de982bba60c1b3cca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e82acd3405ed47dcacd0d1583963b909",
       "IPY_MODEL_bfaa2aba830a422c96d7e3a2ce4acae8",
       "IPY_MODEL_eca2598e806e41fdab2539ed72f80e58"
      ],
      "layout": "IPY_MODEL_2c195c023dcd4ab8987fb3eca3f7c592"
     }
    },
    "12b78775f3ab49679d6db4bfd4188285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0df43236fabd449385bcc5799a3e2ac1",
      "placeholder": "​",
      "style": "IPY_MODEL_dad279ee8658496fa72e796f7bca2121",
      "value": "Downloading data: 100%"
     }
    },
    "130eed6df78f4e568df2d1ed8cb8e6d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "145c7c06f3b7428b8b5a42c7c247bbd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a5b451e03494cad94dbe96c894f2524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ca6044323fc4dfa81d6356f3b22a255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcb0ba80691843798b5e371de5c4fa49",
      "max": 90001,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4b779b171024e7a916f92099af5966b",
      "value": 90001
     }
    },
    "1cf58a726b384f7b8a9e792ed7ee5ec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6dbc347f1aa460c91c50e86e229bf2c",
      "placeholder": "​",
      "style": "IPY_MODEL_b7fdaf8f0b58451a8f20022393c5252e",
      "value": " 92.2k/92.2k [00:00&lt;00:00, 6.86MB/s]"
     }
    },
    "1daedf6b7e6040bda318fb900f00067a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffe69b4f25094eb59506ee16a4d2eceb",
      "placeholder": "​",
      "style": "IPY_MODEL_2834b66bcbb8457389b94c47c475e233",
      "value": "Generating train split: 100%"
     }
    },
    "2077a92ff30049ffa380c44846b1a4df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23548116836246eba5a817378f3222dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2676d305c128479589a93565c779588e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26fa79b8e222484ba6915366b3059672": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2834b66bcbb8457389b94c47c475e233": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "289431d93ad44fcc8b8d6e30f3889beb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2aeb0aa31a2b46a68db2987f84eefb34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c195c023dcd4ab8987fb3eca3f7c592": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30622971c7814fdb899660f789004623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_485cb13f64484c7391f7a6d16d33ad23",
      "max": 18462,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa73585ff1564ca0a28453573cacb69f",
      "value": 18462
     }
    },
    "347b4a53d2e64b1296bb44a34d26a73d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3be60cad87cd45e39eee6d64416c8e12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1c5deefd3ca45f0a4647bbad1e22bb1",
      "max": 1066,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_012d4d9e6dd7490cbda4184eb0f48d90",
      "value": 1066
     }
    },
    "41d29751de0e4c3587c6e98c43331fdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "437e01cdc35f404bbc676125b49b89ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "448449b59aac4b2e97e1e88361f93a73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "459251b662154788a900005160f1f064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45dea46d0b3648739dcca3cde19655d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a16847e8c842492e9bd1825b1ac5284c",
      "max": 3246,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b4f6c3796e4b8aab4d62d4fbad4b27",
      "value": 3246
     }
    },
    "46b4f6c3796e4b8aab4d62d4fbad4b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "485cb13f64484c7391f7a6d16d33ad23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dcf5eb6dacd473b8d8c755c5a1c3fcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e37c3372fa24c00afa959f9cc0ef8f7",
      "max": 74004228,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_add4eae012454a968aa0bf5e8c00d099",
      "value": 74004228
     }
    },
    "52d1d6488d9a4b55ab67ea4589bd0eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5344ee5b34c74eff9797d4f509744053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_723ae4c9989d4676a91b022fe23d12f0",
       "IPY_MODEL_30622971c7814fdb899660f789004623",
       "IPY_MODEL_d55db9e9f41246e88d7dbb5390e7eb91"
      ],
      "layout": "IPY_MODEL_6efa1afa63f74dec8694ac4dba869db2"
     }
    },
    "55ed1b1e90dc416f95cc1e6e539145b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9336d3f18b14eb1ae03ce94e8784212",
      "placeholder": "​",
      "style": "IPY_MODEL_5d4e4c4f64084715990653c66feaf6d5",
      "value": "Generating train split: 100%"
     }
    },
    "563cc5da780b4b3f85ed35e3940bb775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c7c06f3b7428b8b5a42c7c247bbd3",
      "placeholder": "​",
      "style": "IPY_MODEL_26fa79b8e222484ba6915366b3059672",
      "value": " 1066/1066 [00:00&lt;00:00, 48756.09 examples/s]"
     }
    },
    "56872d0a4096420ba74297ce5b8fdcda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55ed1b1e90dc416f95cc1e6e539145b6",
       "IPY_MODEL_4dcf5eb6dacd473b8d8c755c5a1c3fcc",
       "IPY_MODEL_6e2cba35d8194bb0ae95ad52916b925a"
      ],
      "layout": "IPY_MODEL_1a5b451e03494cad94dbe96c894f2524"
     }
    },
    "5d4e4c4f64084715990653c66feaf6d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5deea358c1614eb98e00e0e26b9ef6c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e37c3372fa24c00afa959f9cc0ef8f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7d29575e14434ead8ed64d09338b12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e8624e9541243efa7408a25d8c436e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a0c104eba64f3ab5dd2b69c6300e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff85e6a4c1784d558f370f9b46baefc9",
       "IPY_MODEL_9890948b18604dad86887240c9859d31",
       "IPY_MODEL_af25bf85691240d5a4be7cb0a227aaa9"
      ],
      "layout": "IPY_MODEL_794f64e69e8e49a2a493048cc290b538"
     }
    },
    "61e77f0d3d1043feb118d2eee30b0ef8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62f190ffb66247fda59f0b633beae9b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6440d52bdb834f75940ce372c56f49ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "679ce78a56174bb9be74b2117b2842ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68de9129d69b467c954bac2fe9e584b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a0508863cf346b2b44aa2aa6e323d10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ba3a54733d54151b42040232ec3dd15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e2cba35d8194bb0ae95ad52916b925a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac0165fb453147adbb897ca9b620e101",
      "placeholder": "​",
      "style": "IPY_MODEL_b7d10011a9ec4cc2ab67e44778b32cd8",
      "value": " 74004228/74004228 [17:06&lt;00:00, 84502.14 examples/s]"
     }
    },
    "6efa1afa63f74dec8694ac4dba869db2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "717ffc6a2b314b6f9c656736b9de44dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68de9129d69b467c954bac2fe9e584b5",
      "placeholder": "​",
      "style": "IPY_MODEL_6a0508863cf346b2b44aa2aa6e323d10",
      "value": "validation.parquet: 100%"
     }
    },
    "723ae4c9989d4676a91b022fe23d12f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61e77f0d3d1043feb118d2eee30b0ef8",
      "placeholder": "​",
      "style": "IPY_MODEL_725b98b1edce43689ec7f704a262e6e4",
      "value": "README.md: 100%"
     }
    },
    "725b98b1edce43689ec7f704a262e6e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7350f829600646ec9a0b6993de2e4acd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75d0b30bd69c4f94a5d34f660f2cde04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75e9e4e77e844f0eb5f535402f1d8544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77215c54c0d54345b2113f1bf1e135fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7833d081956147e3a16e7cdebf6e67f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "794f64e69e8e49a2a493048cc290b538": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79a6468e230f41409bbce8a623959b3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2077a92ff30049ffa380c44846b1a4df",
      "max": 8530,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_679ce78a56174bb9be74b2117b2842ed",
      "value": 8530
     }
    },
    "7d4b3dc79e114dba975f8999abe309d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_459251b662154788a900005160f1f064",
      "max": 92206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04e3389a7e054d8ca8aaf0f583e44668",
      "value": 92206
     }
    },
    "7d651f1667f74a38a35c662530ee2161": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81fcb45a1c74434da2261abd71a5302a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "847c9ad4378148fdbf782b6c9d2441d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1daedf6b7e6040bda318fb900f00067a",
       "IPY_MODEL_79a6468e230f41409bbce8a623959b3c",
       "IPY_MODEL_cdad806fa560495bbc4edea1de382c97"
      ],
      "layout": "IPY_MODEL_cc151ab7409b469cade2848294e38a3f"
     }
    },
    "849eaa0f87a54f96ae9e12fcb9f23a2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8630d427eae24c11987029fe54e72092": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88824f06498a4770a205a13a92711f61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8be3ee4139d04a4198e4e964d6755ecd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb99894a782d418fac67e23da1945818",
      "placeholder": "​",
      "style": "IPY_MODEL_7833d081956147e3a16e7cdebf6e67f7",
      "value": " 90.0k/90.0k [00:00&lt;00:00, 5.29MB/s]"
     }
    },
    "8dd70da844404e0093d4fb1c8d8f8a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3ff506573d4e37bb7731e5c456cbaf",
      "placeholder": "​",
      "style": "IPY_MODEL_2676d305c128479589a93565c779588e",
      "value": "train.parquet: 100%"
     }
    },
    "94323561a2bc49cb900a7cee01ec492c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8dd70da844404e0093d4fb1c8d8f8a20",
       "IPY_MODEL_b6b6d325f16343f5afac5b583debbbdb",
       "IPY_MODEL_a173346807904a738070e9c30e5fd51e"
      ],
      "layout": "IPY_MODEL_e685a24dbcfa444d8cbb6ed1abe3b327"
     }
    },
    "9743ce89a11b4d249bbcac13f185ae40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9890948b18604dad86887240c9859d31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b703dc00e0b1485f81e1602899347557",
      "max": 1066,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_81fcb45a1c74434da2261abd71a5302a",
      "value": 1066
     }
    },
    "9ab1e9a090df443486259923b2d88d7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bb3d91de90449bba05a6f4c75dc2d43",
      "placeholder": "​",
      "style": "IPY_MODEL_ee4b27941b994d8a82bedcdc034473c7",
      "value": "test.parquet: 100%"
     }
    },
    "9c34a3b4b0fc48a0a959b08aa5479ff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e773f7e81674cebb65a13332f3bff7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d651f1667f74a38a35c662530ee2161",
      "placeholder": "​",
      "style": "IPY_MODEL_8630d427eae24c11987029fe54e72092",
      "value": "bookcorpus.py: 100%"
     }
    },
    "9fa28c2266d749728e4339c41fcd9f19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a16847e8c842492e9bd1825b1ac5284c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a173346807904a738070e9c30e5fd51e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_130eed6df78f4e568df2d1ed8cb8e6d8",
      "placeholder": "​",
      "style": "IPY_MODEL_2aeb0aa31a2b46a68db2987f84eefb34",
      "value": " 699k/699k [00:00&lt;00:00, 10.9MB/s]"
     }
    },
    "a4b779b171024e7a916f92099af5966b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a65e8b147e7d4c5b89832545dbec51c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b76b665fd16a4a4fb0213c41abbdc424",
       "IPY_MODEL_3be60cad87cd45e39eee6d64416c8e12",
       "IPY_MODEL_563cc5da780b4b3f85ed35e3940bb775"
      ],
      "layout": "IPY_MODEL_6440d52bdb834f75940ce372c56f49ad"
     }
    },
    "ac0165fb453147adbb897ca9b620e101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "add4eae012454a968aa0bf5e8c00d099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af25bf85691240d5a4be7cb0a227aaa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7350f829600646ec9a0b6993de2e4acd",
      "placeholder": "​",
      "style": "IPY_MODEL_75d0b30bd69c4f94a5d34f660f2cde04",
      "value": " 1066/1066 [00:00&lt;00:00, 47722.07 examples/s]"
     }
    },
    "afc97ed693cd4bcbbdc3c5027eb69b77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b12f8f2c9eb44899b9598b20b9307356": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e773f7e81674cebb65a13332f3bff7d",
       "IPY_MODEL_45dea46d0b3648739dcca3cde19655d3",
       "IPY_MODEL_e0bf6a12069748ce9b72269911f87e35"
      ],
      "layout": "IPY_MODEL_afc97ed693cd4bcbbdc3c5027eb69b77"
     }
    },
    "b19888718df942a8b98595c6691d514c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12b78775f3ab49679d6db4bfd4188285",
       "IPY_MODEL_d9fa2a28a41b44119cb04425bc0626fc",
       "IPY_MODEL_f1793cabd34242b984edba931196576a"
      ],
      "layout": "IPY_MODEL_fc8543f3def74738aeaf25ab4702dae0"
     }
    },
    "b19f846bb67343e9a2dbe3fff6518ca5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b6d325f16343f5afac5b583debbbdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce69b49e00d143f6b81d7f2d06fec378",
      "max": 698845,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4daefb22efc47b3bdb607512147cd81",
      "value": 698845
     }
    },
    "b703dc00e0b1485f81e1602899347557": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76b665fd16a4a4fb0213c41abbdc424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c168c3c6550845f18a60b55506112f2a",
      "placeholder": "​",
      "style": "IPY_MODEL_eb350264d7df44508d909ed59342c38a",
      "value": "Generating test split: 100%"
     }
    },
    "b7d10011a9ec4cc2ab67e44778b32cd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7fdaf8f0b58451a8f20022393c5252e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9336d3f18b14eb1ae03ce94e8784212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3ff506573d4e37bb7731e5c456cbaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfaa2aba830a422c96d7e3a2ce4acae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba3a54733d54151b42040232ec3dd15",
      "max": 7457,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_347b4a53d2e64b1296bb44a34d26a73d",
      "value": 7457
     }
    },
    "c168c3c6550845f18a60b55506112f2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1c5deefd3ca45f0a4647bbad1e22bb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc151ab7409b469cade2848294e38a3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdad806fa560495bbc4edea1de382c97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_289431d93ad44fcc8b8d6e30f3889beb",
      "placeholder": "​",
      "style": "IPY_MODEL_9743ce89a11b4d249bbcac13f185ae40",
      "value": " 8530/8530 [00:00&lt;00:00, 4534.52 examples/s]"
     }
    },
    "ce69b49e00d143f6b81d7f2d06fec378": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4daefb22efc47b3bdb607512147cd81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d55db9e9f41246e88d7dbb5390e7eb91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41d29751de0e4c3587c6e98c43331fdf",
      "placeholder": "​",
      "style": "IPY_MODEL_849eaa0f87a54f96ae9e12fcb9f23a2b",
      "value": " 18.5k/18.5k [00:00&lt;00:00, 621kB/s]"
     }
    },
    "d9fa2a28a41b44119cb04425bc0626fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52d1d6488d9a4b55ab67ea4589bd0eb6",
      "max": 1179510242,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_437e01cdc35f404bbc676125b49b89ec",
      "value": 1179510242
     }
    },
    "dad279ee8658496fa72e796f7bca2121": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0bf6a12069748ce9b72269911f87e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c34a3b4b0fc48a0a959b08aa5479ff7",
      "placeholder": "​",
      "style": "IPY_MODEL_75e9e4e77e844f0eb5f535402f1d8544",
      "value": " 3.25k/3.25k [00:00&lt;00:00, 110kB/s]"
     }
    },
    "e685a24dbcfa444d8cbb6ed1abe3b327": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6dbc347f1aa460c91c50e86e229bf2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e82acd3405ed47dcacd0d1583963b909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23548116836246eba5a817378f3222dd",
      "placeholder": "​",
      "style": "IPY_MODEL_5e8624e9541243efa7408a25d8c436e2",
      "value": "README.md: 100%"
     }
    },
    "eb350264d7df44508d909ed59342c38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb99894a782d418fac67e23da1945818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eca2598e806e41fdab2539ed72f80e58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77215c54c0d54345b2113f1bf1e135fb",
      "placeholder": "​",
      "style": "IPY_MODEL_9fa28c2266d749728e4339c41fcd9f19",
      "value": " 7.46k/7.46k [00:00&lt;00:00, 455kB/s]"
     }
    },
    "ee4b27941b994d8a82bedcdc034473c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1793cabd34242b984edba931196576a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b19f846bb67343e9a2dbe3fff6518ca5",
      "placeholder": "​",
      "style": "IPY_MODEL_5deea358c1614eb98e00e0e26b9ef6c2",
      "value": " 1.18G/1.18G [00:23&lt;00:00, 51.0MB/s]"
     }
    },
    "f43082c8ea3e4642bc7b22b4a3a0f93f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ab1e9a090df443486259923b2d88d7f",
       "IPY_MODEL_7d4b3dc79e114dba975f8999abe309d5",
       "IPY_MODEL_1cf58a726b384f7b8a9e792ed7ee5ec2"
      ],
      "layout": "IPY_MODEL_5e7d29575e14434ead8ed64d09338b12"
     }
    },
    "fa73585ff1564ca0a28453573cacb69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc8543f3def74738aeaf25ab4702dae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcb0ba80691843798b5e371de5c4fa49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcf263ee4e504219b0036a8d04c78fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_717ffc6a2b314b6f9c656736b9de44dd",
       "IPY_MODEL_1ca6044323fc4dfa81d6356f3b22a255",
       "IPY_MODEL_8be3ee4139d04a4198e4e964d6755ecd"
      ],
      "layout": "IPY_MODEL_62f190ffb66247fda59f0b633beae9b3"
     }
    },
    "ff85e6a4c1784d558f370f9b46baefc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_448449b59aac4b2e97e1e88361f93a73",
      "placeholder": "​",
      "style": "IPY_MODEL_88824f06498a4770a205a13a92711f61",
      "value": "Generating validation split: 100%"
     }
    },
    "ffe69b4f25094eb59506ee16a4d2eceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
